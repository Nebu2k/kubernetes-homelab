# Kubernetes Homelab - GitOps with ArgoCD

Production-ready K3s cluster managed via GitOps using ArgoCD App-of-Apps pattern.

## üéØ Architecture

**Key Principle**: ArgoCD manages everything EXCEPT itself (prevents self-management conflicts).

### Deployment Flow

```text
1. Manual: K3s + Kube-VIP ‚Üí HA Control Plane
2. Manual: ArgoCD via Helm ‚Üí GitOps Engine
3. GitOps: bootstrap/root-app.yaml ‚Üí App-of-Apps
4. GitOps: Everything else deployed automatically with Sync-Waves
```

### Sync-Wave Order

| Wave | Component |
|------|-----------|
{% for wave, components in sync_waves.items() -%}
| {{ wave }} | {{ components | join(', ') | title | replace('-', ' ') }} |
{% endfor %}
## üìÅ Repository Structure

```text
{{ repo_structure }}
```

## üöÄ Fresh Installation

### Prerequisites

- 2+ nodes
- Domain with Cloudflare DNS
- Cloudflare API Token (Zone.DNS Edit permission)
- S3-compatible storage for Longhorn backups (optional)

### Step 1: Install K3s Cluster (**on raspi4**)

**First control plane node:**

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=stable sh -s - server \
  --cluster-init \
  --tls-san 192.168.2.249 \
  --tls-san raspi4 \
  --tls-san 192.168.2.2 \
  --disable traefik \
  --write-kubeconfig-mode 644

# Save token for additional nodes
sudo cat /var/lib/rancher/k3s/server/node-token
```

### Step 2: Install Kube-VIP (**on raspi4** - Control Plane HA)

```bash
kubectl apply -f https://kube-vip.io/manifests/rbac.yaml

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-vip-ds
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: kube-vip-ds
  template:
    metadata:
      labels:
        name: kube-vip-ds
    spec:
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/control-plane: "true"
      serviceAccountName: kube-vip
      containers:
      - name: kube-vip
        image: ghcr.io/kube-vip/kube-vip:v1.0.1
        args: ["manager"]
        env:
        - name: vip_arp
          value: "true"
        - name: port
          value: "6443"
        - name: vip_cidr
          value: "32"
        - name: cp_enable
          value: "true"
        - name: cp_namespace
          value: kube-system
        - name: vip_leaderelection
          value: "true"
        - name: address
          value: "192.168.2.249"  # TODO: Your VIP
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
EOF

# Wait for Kube-VIP to be ready
sleep 10

# Test VIP
ping -c 3 192.168.2.249
```

### Step 3: Configure kubectl with VIP (**on your laptop**)

```bash
scp raspi4:/etc/rancher/k3s/k3s.yaml ~/.kube/config
sed -i '' 's/127.0.0.1/192.168.2.249/g' ~/.kube/config
kubectl get nodes
```

### Step 4: Join Additional Control Plane Nodes (**on raspi5**)

```bash
# Join via VIP (not node IP!)
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=stable sh -s - server \
  --server https://192.168.2.249:6443 \
  --token <token-from-step-1> \
  --tls-san 192.168.2.249 \
  --tls-san raspi5 \
  --tls-san 192.168.2.9 \
  --disable traefik \
  --write-kubeconfig-mode 644
```

### Step 4.5: Join Worker Nodes with Longhorn Storage (**on k3s-worker-1**)

**Prerequisites:**

- Second disk installed (e.g., 2TB NVMe for Longhorn storage)
- Static IP configured via DHCP reservation in router

**1. Install system essentials:**

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y curl wget git vim htop net-tools iptables qemu-guest-agent
sudo systemctl start qemu-guest-agent
```

**2. Prepare second disk for Longhorn:**

```bash
# Identify disk (usually /dev/sdb for second disk)
lsblk

# Format disk with ext4
sudo mkfs.ext4 -L longhorn-storage /dev/sdb

# Create Longhorn mountpoint
sudo mkdir -p /var/lib/longhorn

# Get UUID for permanent mounting
sudo blkid /dev/sdb

# Add to fstab (replace <uuid> with actual UUID from blkid)
echo "UUID=<uuid> /var/lib/longhorn ext4 defaults,noatime 0 2" | sudo tee -a /etc/fstab

# Mount and verify
sudo mount -a
df -h /var/lib/longhorn
```

**3. Join as K3s worker:**

```bash
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.2.249:6443 \
  K3S_TOKEN=<token-from-step-1> \
  sh -
```

‚ö†Ô∏è **For Multipass VMs with multiple network interfaces:**

If the VM has both a NAT interface (e.g., 192.168.64.x) and a bridged interface (e.g., 192.168.2.x), explicitly specify the correct interface:

```bash
curl -sfL https://get.k3s.io | K3S_URL=https://192.168.2.249:6443 \
  K3S_TOKEN=<token-from-step-1> \
  INSTALL_K3S_EXEC="--node-ip=<bridged-ip> --flannel-iface=ens4" \
  sh -
```

Replace `<bridged-ip>` with the IP from your cluster network (e.g., 192.168.2.x) and adjust `ens4` to match your bridged interface name (check with `ip addr show`).

**4. Label node as worker (from your laptop):**

```bash
kubectl label node k3s-worker-1 node-role.kubernetes.io/worker=worker
```

**5. Verify Longhorn detects storage:**

```bash
# From your laptop
kubectl get nodes
kubectl get pods -n longhorn-system -o wide | grep k3s-worker-1

# Check Longhorn UI (http://longhorn.elmstreet79.de)
# Node ‚Üí k3s-worker-1 ‚Üí should show full disk capacity
```

‚ö†Ô∏è **Note:** Longhorn automatically detects `/var/lib/longhorn` - no additional configuration needed!

### Step 5: Install ArgoCD via Helm (**on your laptop**)

‚ö†Ô∏è **ArgoCD is NOT managed via GitOps** (prevents self-management conflicts)

```bash
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Install with your domain
# Note: Installs latest version (no --version flag)
helm install argocd argo/argo-cd \
  --namespace argocd \
  --create-namespace \
  --set global.domain=argocd.elmstreet79.de \
  --set configs.cm.url=https://argocd.elmstreet79.de \
  --set 'configs.params.server\.insecure'=true

# Wait for ArgoCD to be ready
kubectl wait --for=condition=available --timeout=300s \
  deployment/argocd-server -n argocd

# Get admin password
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath="{.data.password}" | base64 -d && echo
```

**Why `server.insecure=true`?**

- ArgoCD runs on HTTP internally
- NGINX Ingress terminates TLS
- Prevents redirect loops

**Note:** If you changed the domain in Step 5, update the Helm values accordingly.

### Step 6: Fork & Configure Repository (**on your laptop**)

```bash
# Fork https://github.com/Nebu2k/kubernetes-homelab
git clone https://github.com/YOUR_USERNAME/kubernetes-homelab
cd kubernetes-homelab

# Install Git hooks (enables auto-README regeneration on commit)
.githooks/install.sh
```

**‚öôÔ∏è Configure for your environment:**

> ‚ö†Ô∏è **Warning:** The repository is pre-configured for `elmstreet79.de`. If using your own domain, update:

1. **MetalLB IP Pool** (adjust to your network):

   ```bash
   vim overlays/production/metallb/metallb-ip-pool.yaml
   # Change: 192.168.2.250-192.168.2.254
   ```

2. **Cert-Manager Email**:

   ```bash
   vim overlays/production/cert-manager/cluster-issuer.yaml
   # Change: certs@elmstreet79.de
   ```

3. **Cloudflare API Token** (required):

   ```bash
   # Create from example
   cp overlays/production/cert-manager/cloudflare-token-unsealed.yaml.example \
      overlays/production/cert-manager/cloudflare-token-unsealed.yaml
   
   # Add your Cloudflare API token
   vim overlays/production/cert-manager/cloudflare-token-unsealed.yaml
   # Change: api-token: "your-cloudflare-api-token-here"
   
   # Note: Sealing happens AFTER cluster bootstrap (Step 7+)
   # For now, keep it unsealed locally (gitignored)
   ```

4. **Longhorn S3 Backup** (optional):

   ```bash
   # Create from example
   cp overlays/production/longhorn/s3-secret-unsealed.yaml.example \
      overlays/production/longhorn/s3-secret-unsealed.yaml
   
   # Update MinIO/S3 credentials
   vim overlays/production/longhorn/s3-secret-unsealed.yaml
   # Change: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_ENDPOINTS
   
   # Note: Sealing happens AFTER cluster bootstrap (Step 7+)
   # For now, keep it unsealed locally (gitignored)
   ```

   ‚ö†Ô∏è **Note:** `*-unsealed.yaml` files are gitignored for security. Only `.example` templates are committed.

5. **Cloudflare DNS Sync Configuration** (optional - adjust DynDNS target):

   If you want to change the DynDNS target for public DNS records:

   ```bash
   vim overlays/production/cert-manager/cloudflare-dns-sync-configmap.yaml
   # Update TARGET to your DynDNS hostname (e.g., your-hostname.ipv64.net)
   # Update ZONE_ID to your Cloudflare Zone ID
   ```

   **Get Cloudflare Zone ID** (if needed):
   - Cloudflare Dashboard ‚Üí Your Domain ‚Üí Overview (right sidebar)

**Commit and push:**

```bash
git add -A
git commit -m "Configure for my environment"
git push
```

### Step 7: Bootstrap GitOps (**on your laptop**)

```bash
# Deploy App-of-Apps
kubectl apply -f bootstrap/root-app.yaml

# Watch ArgoCD deploy everything (~5-10 minutes)
kubectl get applications -n argocd -w
```

**What happens:**

- Sealed Secrets Controller installs first (Sync-Wave 0)
- MetalLB, Cert-Manager, NGINX, etc. follow in order
- Some apps will stay "Progressing" until secrets are sealed (next step)

### Step 7.5: Seal Secrets (**on your laptop** - AFTER Step 7)

‚ö†Ô∏è **Wait until Sealed Secrets Controller is ready:**

```bash
# Check if controller is running
kubectl wait --for=condition=available --timeout=300s \
  deployment/sealed-secrets-controller -n kube-system

# Now seal your secrets
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/cert-manager/cloudflare-token-unsealed.yaml \
  > overlays/production/cert-manager/cloudflare-token-sealed.yaml

# Seal AdGuard credentials for DNS sync:
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/private-services/adguard-credentials-unsealed.yaml \
  > overlays/production/private-services/adguard-credentials-sealed.yaml

# If using Longhorn S3 backup:
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/longhorn/s3-secret-unsealed.yaml \
  > overlays/production/longhorn/s3-secret-sealed.yaml

# Seal Victoria Metrics Grafana admin credentials:
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/victoria-metrics/grafana-admin-unsealed.yaml \
  > overlays/production/victoria-metrics/grafana-admin-sealed.yaml

# Commit and push
git add overlays/production/*/kustomization.yaml
git add overlays/production/*/*-sealed.yaml
git commit -m "üîê Add sealed secrets"
git push

# ArgoCD will auto-sync and apply the secrets
kubectl get applications -n argocd -w
```

### Step 7.6: Configure Homepage Widgets (**on your laptop** - AFTER Grafana is ready)

**Create sealed secrets for Homepage widgets:**

‚ö†Ô∏è **Important:** For each widget secret, copy the example file, edit with your credentials, then seal it.

```bash
{% for secret in homepage_secrets %}
# {{ loop.index }}. {{ secret.base_name | replace('-', ' ') | title }}
cp overlays/production/homepage/{{ secret.example_file }} \
   overlays/production/homepage/{{ secret.unsealed_file }}

# Edit the file and replace placeholder values with your actual credentials
vim overlays/production/homepage/{{ secret.unsealed_file }}

# Seal the secret
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/homepage/{{ secret.unsealed_file }} \
  > overlays/production/homepage/{{ secret.sealed_file }}

{% endfor %}
# Commit and push
git add overlays/production/homepage/*-sealed.yaml
git commit -m "Add Homepage widget credentials"
git push
```

‚ö†Ô∏è **Note:** If you rebuild the cluster, passwords may change (e.g., Grafana admin password), so you'll need to recreate the affected sealed secrets.

### Step 8: Verify Deployment (**on your laptop**)

```bash
# All apps should be Synced + Healthy
kubectl get applications -n argocd

# MetalLB assigned LoadBalancer IP
kubectl get svc -n ingress-nginx
# EXTERNAL-IP should show 192.168.2.250-254 range

# Certificates issued (takes 2-5 min for DNS-01)
kubectl get certificate -A
# All should show READY=True

# Ingresses configured
kubectl get ingress -A
```

### Step 9: Access UIs (**from your laptop browser**)

‚ö†Ô∏è **DNS Records**: All public DNS records (HTTPS services) are **automatically created** by the Cloudflare DNS Sync job after deployment! No manual DNS configuration needed.

**ArgoCD:**

```text
URL: https://argocd.elmstreet79.de
User: admin
Pass: <from-step-5>
```

‚ö†Ô∏è **Change password immediately!**

1. User Info ‚Üí Update Password
2. Then delete initial secret:

```bash
kubectl -n argocd delete secret argocd-initial-admin-secret
```

**Portainer:**

```text
URL: https://portainer.elmstreet79.de
```

‚ö†Ô∏è **Create admin account within 5 minutes!**

If timeout, restart the pod:

```bash
kubectl delete pod -n portainer -l app.kubernetes.io/name=portainer
```

**Longhorn:**

```text
URL: http://longhorn.elmstreet79.de
(Internal DNS only, managed by AdGuard DNS Sync)
```

**Victoria Metrics Monitoring Stack:**

```text
Grafana:        https://grafana.elmstreet79.de
Prometheus API: http://<node-ip>:8428 (vmsingle - internal only)
VMAgent:        Scrapes metrics from all Kubernetes components
VMAlert:        Evaluates alerting rules (blackhole mode)
```

**Get Grafana admin password:**

```bash
kubectl get secret -n monitoring grafana-admin-credentials \
  -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

Default user: `admin`

üìä **Pre-installed components:**

- **VictoriaMetrics Single**: Time-series database (30d retention, 15Gi storage)
- **VMAgent**: Metric collection from Kubernetes components (kubelet, apiserver, etcd, etc.)
- **Grafana**: Pre-configured with Victoria Metrics datasource and Kubernetes dashboards
- **Node Exporter**: Hardware metrics from all nodes
- **Kube State Metrics**: Kubernetes object metrics

‚ö° **Victoria Metrics advantages over Prometheus:**

- Lower resource usage (CPU & memory)
- Faster queries on large datasets
- Better compression (less disk space)
- 100% Prometheus-compatible (drop-in replacement)

üîí **Security note:** VictoriaMetrics API is not exposed publicly (no ingress). Access via Grafana datasource or port-forward:

```bash
kubectl port-forward -n monitoring svc/vm-vmsingle 8428:8428
```

**Homepage (Homelab Dashboard):**

```text
URL: https://home.elmstreet79.de
```

üè† **Features:** Unified dashboard with links to all services, real-time Kubernetes cluster metrics, auto-discovery of ingresses, dark theme, widgets for Proxmox/ArgoCD/Grafana

**Uptime Kuma (Uptime Monitoring):**

```text
URL: https://uptime.elmstreet79.de
```

‚ö†Ô∏è **First visit:** Create admin account on initial access. Then add monitors for your services.

**Private Services:**

Private Services provide internal DNS names for services running outside Kubernetes (e.g., Docker containers on Raspberry Pis) without exposing them publicly. This architecture uses:

1. **Internal DNS Names**: Services accessible via `*.elmstreet79.de` only within the local network
2. **Automated DNS Management**: PostSync Hook + CronJob automatically sync Kubernetes Ingresses to AdGuard Home DNS Rewrites
3. **Single Source of Truth**: AdGuard Home manages all internal DNS rewrites, CoreDNS forwards queries to AdGuard
4. **No Public SSL**: Internal services use HTTP only (no cert-manager annotations)

**Architecture:**

```text
External Service ‚Üí Kubernetes Service (ClusterIP) ‚Üí Manual Endpoints ‚Üí External IP:Port
                ‚Üì
            Ingress (no cert-manager) ‚Üí NGINX LoadBalancer (192.168.2.250)
                ‚Üì
            AdGuard DNS Sync Job ‚Üí AdGuard API ‚Üí DNS Rewrite (*.elmstreet79.de ‚Üí 192.168.2.250)
                ‚Üì
            CoreDNS ‚Üí Forward *.elmstreet79.de ‚Üí AdGuard DNS (192.168.2.2, 192.168.2.4)
```

**Examples:**

```text
AdGuard Home: http://adguard.elmstreet79.de ‚Üí 192.168.2.2:8080
Beszel Monitor: http://beszel.elmstreet79.de ‚Üí 192.168.2.9:8090
MinIO Console: http://minio.elmstreet79.de ‚Üí 192.168.2.9:9393
MinIO API: http://minio-api.elmstreet79.de ‚Üí 192.168.2.9:9300
Longhorn: http://longhorn.elmstreet79.de ‚Üí Internal K8s service
```

**AdGuard DNS Sync Automation:**

- **PostSync Hook**: Runs automatically after every ArgoCD sync
- **Filter Logic**: Only syncs Ingresses WITHOUT `cert-manager.io/cluster-issuer` annotation
- **AdGuard API**: Creates/updates/deletes DNS Rewrites pointing to NGINX LoadBalancer IP (192.168.2.250)
- **Auto-Cleanup**: Removes orphaned DNS entries when Ingresses are deleted

**Public Services with SSL:**

Public services (accessible from the internet) use a separate DNS sync system:

**Cloudflare DNS Sync Automation:**

- **PostSync Hook**: Runs automatically after every ArgoCD sync
- **CronJob**: Fallback every 6 hours
- **Filter Logic**: Only syncs Ingresses WITH `cert-manager.io/cluster-issuer` annotation
- **Cloudflare API**: Creates/updates/deletes CNAME records pointing to DynDNS target (e.g., `nebu2k.ipv64.net`)
- **Auto-Cleanup**: Removes orphaned DNS records when Ingresses are deleted
- **Let's Encrypt**: Cert-Manager automatically provisions SSL certificates via DNS-01 challenge

**Examples:**

```text
TeslaLogger: https://teslalogger.elmstreet79.de (‚Üí 192.168.2.9:3000)
Dreambox: https://dreambox.elmstreet79.de (‚Üí 192.168.2.11:80)
(External services routed via NGINX Ingress with TLS)
```

### Internal CA for Private Services

The cluster includes an **Internal Certificate Authority** for issuing self-signed certificates to private services (services without public SSL certificates). This is automatically deployed via `private-services.yaml` (Sync-Wave 16).

**Architecture:**

1. **Self-Signed CA**: Created by cert-manager in `cert-manager` namespace
2. **Internal ClusterIssuer**: Issues certificates for internal services
3. **Automatic Distribution**: CronJob copies CA certificate to `homepage` namespace every 12 hours
4. **Use Case**: Homepage can trust internal services with self-signed certificates

**Trust Internal CA on your laptop** (for accessing internal HTTPS services without browser warnings):

```bash
# Export the CA certificate from the cluster
kubectl get secret internal-ca-secret -n cert-manager \
  -o jsonpath='{.data.ca\.crt}' | base64 -d > internal-ca.crt

# macOS: Add to system keychain and trust
sudo security add-trusted-cert -d -r trustRoot \
  -k /Library/Keychains/System.keychain internal-ca.crt

# Linux: Add to trusted certificates
sudo cp internal-ca.crt /usr/local/share/ca-certificates/
sudo update-ca-certificates

# Windows: Import via certmgr.msc
# Double-click internal-ca.crt ‚Üí Install Certificate ‚Üí Local Machine ‚Üí 
# Place in "Trusted Root Certification Authorities"
```

‚ö†Ô∏è **Note:** After adding the CA certificate, restart your browser to apply the changes.

**Verify CA is working:**

```bash
# Check if internal-ca certificate exists
kubectl get certificate internal-ca -n cert-manager

# Check if ClusterIssuer is ready
kubectl get clusterissuer internal-ca-issuer
```

## üîß Management

### View Application Status

```bash
kubectl get applications -n argocd
kubectl describe application <app-name> -n argocd
```

### Update Component

```bash
# Edit Helm values
vim base/nginx-ingress/values.yaml

# Commit and push - ArgoCD auto-syncs
git add base/nginx-ingress/values.yaml
git commit -m "Update NGINX to 2 replicas"
git push

# Watch sync
kubectl get application nginx-ingress -n argocd -w
```

### Update Secrets

```bash
# 1. Edit unsealed secret
vim overlays/production/cert-manager/cloudflare-token-unsealed.yaml

# 2. Re-seal
kubeseal --format=yaml --controller-namespace=kube-system \
  < overlays/production/cert-manager/cloudflare-token-unsealed.yaml \
  > overlays/production/cert-manager/cloudflare-token-sealed.yaml

# 3. Commit and push
git add overlays/production/cert-manager/cloudflare-token-sealed.yaml
git commit -m "Rotate Cloudflare token"
git push
```

### Force Sync Application

```bash
# Via kubectl
kubectl patch application <app-name> -n argocd --type merge \
  -p '{"operation":{"initiatedBy":{"username":"manual"}}}'

# Via ArgoCD CLI
argocd app sync <app-name>
```

### Hard Refresh Application

Sometimes ArgoCD needs a complete reset (e.g., stuck state, CRD issues):

```bash
# Delete and re-create the ArgoCD Application (doesn't delete K8s resources)
kubectl delete application <app-name> -n argocd && sleep 2 && \
  kubectl apply -f apps/<app-name>.yaml

# Example: Hard refresh Longhorn
kubectl delete application longhorn -n argocd && sleep 2 && \
  kubectl apply -f apps/longhorn.yaml
```

**Note:** This only resets the ArgoCD Application object, not the actual Kubernetes resources. Useful for clearing stuck sync states or comparison errors.

## üêõ Troubleshooting

### MetalLB Not Assigning IPs

**Check:**

```bash
kubectl logs -n metallb-system -l app.kubernetes.io/component=speaker
```

**Should NOT show:**

```text
"error":"assigned IP not allowed by config"
```

**Fix:** Ensure L2Advertisement exists in `overlays/production/metallb/metallb-ip-pool.yaml`

### Certificates Not Ready

**Check status:**

```bash
kubectl describe certificate <name> -n <namespace>
kubectl get challenge -A
```

**Common issues:**

1. Cloudflare secret not sealed correctly
2. DNS-01 challenge takes 2-5 minutes (normal)
3. Cert-Manager webhook TLS error (delete webhook pod to restart)

**Fix webhook:**

```bash
kubectl delete pod -n cert-manager -l app.kubernetes.io/name=webhook
```

### ArgoCD App OutOfSync

**Check:**

```bash
kubectl describe application <app-name> -n argocd
kubectl logs -n argocd deployment/argocd-application-controller
```

**Force refresh:**

```bash
argocd app sync <app-name> --force
```

### Grafana Widget Not Showing on Homepage

**Check:**

```bash
# 1. Verify Homepage has the environment variables
kubectl get deployment homepage -n homepage -o yaml | grep -E "GRAFANA_(USERNAME|PASSWORD)"

# 2. Check if secret exists and has correct fields
kubectl get secret homepage-grafana -n homepage -o yaml

# 3. Check Homepage logs
kubectl logs -n homepage deployment/homepage

# 4. Verify Grafana credentials work
GRAFANA_USER=$(kubectl get secret homepage-grafana -n homepage -o jsonpath='{.data.username}' | base64 -d)
GRAFANA_PASS=$(kubectl get secret homepage-grafana -n homepage -o jsonpath='{.data.password}' | base64 -d)
kubectl exec -n monitoring deployment/victoria-metrics-k8s-stack-grafana -- \
  wget -q -O- http://$GRAFANA_USER:$GRAFANA_PASS@localhost:3000/api/admin/stats
```

**Fix if credentials are invalid:**

Re-create the sealed secret with current Grafana admin password as described in Step 7.6.

### Unseal a Sealed Secret (for debugging)

If you need to view the decrypted content of a sealed secret:

```bash
# Get the sealed secret from the cluster
kubectl get sealedsecret <sealed-secret-name> -n <namespace> -o yaml > sealed.yaml

# The sealed secret will be automatically decrypted by the controller and stored as a regular Secret
# View the decrypted secret
kubectl get secret <secret-name> -n <namespace> -o yaml

# Decode a specific field (e.g., password)
kubectl get secret <secret-name> -n <namespace> -o jsonpath='{.data.password}' | base64 -d

# Example: View Grafana admin password
kubectl get secret -n monitoring grafana-admin-credentials \
  -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

‚ö†Ô∏è **Note:** You cannot "unseal" the encrypted data in the SealedSecret YAML file without access to the cluster's private key. The Sealed Secrets controller automatically decrypts SealedSecrets into regular Secrets when applied to the cluster.

## üìö Components

| Component | Version | Purpose |
|-----------|---------|---------|
{% for name, info in versions.items() -%}
| {{ info.chart | title | replace('-', ' ') }} | {{ info.version }} | {{ name | replace('-', ' ') | title }} |
{% endfor -%}
| K3s | v1.33.5 | Lightweight Kubernetes |
| Kube-VIP | v1.0.1 | Control plane HA |

## üìñ Documentation

{% for name, url in documentation_links.items() -%}
- [{{ name }}]({{ url }})
{% endfor %}
## üìù License

MIT

---

> ü§ñ **This README is auto-generated** using `docs-generator/generate_readme.py`  
> To regenerate manually: `make docs`  
> Auto-generation on commit: Enabled via `.githooks/pre-commit` (run `.githooks/install.sh` after clone)

