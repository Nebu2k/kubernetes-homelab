# Kubernetes Homelab - GitOps with ArgoCD

Production-ready K3s cluster managed via GitOps using ArgoCD App-of-Apps pattern.

## ğŸ¯ Architecture

**Key Principle**: ArgoCD manages everything EXCEPT itself (prevents self-management conflicts).

### Deployment Flow

```
1. Manual: K3s + Kube-VIP â†’ HA Control Plane
2. Manual: ArgoCD via Helm â†’ GitOps Engine
3. GitOps: bootstrap/root-app.yaml â†’ App-of-Apps
4. GitOps: Everything else deployed automatically with Sync-Waves
```

### Sync-Wave Order

| Wave | Component | Purpose |
|------|-----------|---------|
| 0 | Sealed Secrets | Decrypt secrets |
| 1 | MetalLB | LoadBalancer IPs |
| 2 | Cert-Manager | TLS certificates |
| 3 | NGINX Ingress | HTTP(S) routing |
| 4 | Longhorn | Persistent storage |
| 5 | Portainer | Management UI |
| 10+ | Configs | Component-specific configs (ClusterIssuers, Ingresses, etc.) |

## ğŸ“ Repository Structure

```
homelab/
â”œâ”€â”€ bootstrap/
â”‚   â””â”€â”€ root-app.yaml              # App-of-Apps (deploys everything)
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ kustomization.yaml         # List of all apps
â”‚   â”œâ”€â”€ sealed-secrets.yaml        # Wave 0
â”‚   â”œâ”€â”€ metallb.yaml               # Wave 1
â”‚   â”œâ”€â”€ metallb-config.yaml        # Wave 10
â”‚   â”œâ”€â”€ cert-manager.yaml          # Wave 2
â”‚   â”œâ”€â”€ cert-manager-config.yaml   # Wave 10
â”‚   â”œâ”€â”€ nginx-ingress.yaml         # Wave 3
â”‚   â”œâ”€â”€ nginx-ingress-config.yaml  # Wave 11
â”‚   â”œâ”€â”€ longhorn.yaml              # Wave 4
â”‚   â”œâ”€â”€ longhorn-config.yaml       # Wave 13
â”‚   â”œâ”€â”€ portainer.yaml             # Wave 5
â”‚   â””â”€â”€ portainer-config.yaml      # Wave 12
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ metallb/values.yaml        # Generic Helm values
â”‚   â”œâ”€â”€ cert-manager/values.yaml
â”‚   â”œâ”€â”€ nginx-ingress/values.yaml
â”‚   â”œâ”€â”€ longhorn/values.yaml
â”‚   â””â”€â”€ portainer/values.yaml
â””â”€â”€ overlays/production/
    â”œâ”€â”€ metallb/
    â”‚   â””â”€â”€ metallb-ip-pool.yaml   # IPAddressPool + L2Advertisement
    â”œâ”€â”€ cert-manager/
    â”‚   â”œâ”€â”€ cluster-issuer.yaml    # Let's Encrypt issuers
    â”‚   â””â”€â”€ cloudflare-token-*.yaml
    â”œâ”€â”€ nginx-ingress/
    â”‚   â””â”€â”€ custom-headers.yaml    # Security headers ConfigMap
    â”œâ”€â”€ portainer/
    â”‚   â””â”€â”€ ingress.yaml           # Portainer HTTPS ingress
    â””â”€â”€ longhorn/
        â””â”€â”€ s3-secret-*.yaml       # S3 backup credentials
```

## ğŸš€ Fresh Installation

### Prerequisites

- 2+ nodes (Raspberry Pi or other)
- Domain with Cloudflare DNS
- Cloudflare API Token (Zone.DNS Edit permission)
- S3-compatible storage for Longhorn backups (optional)

### Step 1: Install K3s Cluster

**First control plane node:**

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -s - server \
  --cluster-init \
  --tls-san 192.168.2.249 \
  --tls-san raspi4 \
  --tls-san 192.168.2.2 \
  --disable traefik \
  --write-kubeconfig-mode 644

# Save token for additional nodes
sudo cat /var/lib/rancher/k3s/server/node-token
```

### Step 2: Install Kube-VIP (Control Plane HA)

```bash
kubectl apply -f https://kube-vip.io/manifests/rbac.yaml

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-vip-ds
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: kube-vip-ds
  template:
    metadata:
      labels:
        name: kube-vip-ds
    spec:
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/control-plane: "true"
      serviceAccountName: kube-vip
      containers:
      - name: kube-vip
        image: ghcr.io/kube-vip/kube-vip:v1.0.1
        args: ["manager"]
        env:
        - name: vip_arp
          value: "true"
        - name: port
          value: "6443"
        - name: vip_cidr
          value: "32"
        - name: cp_enable
          value: "true"
        - name: cp_namespace
          value: kube-system
        - name: vip_leaderelection
          value: "true"
        - name: address
          value: "192.168.2.249"  # TODO: Your VIP
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
